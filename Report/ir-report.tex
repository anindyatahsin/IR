\documentclass[article]{IEEEtran}
\usepackage{cite}
\usepackage{color}
\usepackage{alltt}
\usepackage[utf8]{inputenc}
\usepackage{fancyvrb}
\usepackage{array} 
\usepackage{colortbl}
\usepackage{ctable}
\newcounter{tmpc}
\usepackage{mathtools}

\begin{document}
\title{Mapping Readers' Comments Back to Newspaper Articles}
\author{
        \IEEEauthorblockN{Asif Salekin, Md Anindya Prodhan, Muhammad Nur Yanhaona}
        \IEEEauthorblockA{	\\University of Virginia\\
                       		Email: \{as3df, mtp5cx, mny9md\}@virginia.edu}}
\maketitle


\maketitle
\begin{abstract}
Publishing readers' comments has long been an integral part of maintaining integrity in journalism. The evolution of electronic newspapers has greatly enhanced the scope and limit for these comments. So much so that now it is common to have hundreds of comments on popular issues in popular newspapers. The sheer volume of comments, however, introduces new challenges. For general readers it has become nearly impossible to go through all these comments. At the same time, for the newspaper editor it has become difficult highlighting discussions most relevant to an article. 

In this project we worked on automatically filtering and mapping comments that are most relevant to a newspaper article or a portion of the article using information retrieval techniques. To be able to do that we analyse a newspaper article and construct query language models for terms that occurred in its passages. Then we generate queries from those passages and match them against readers' comments treating the latter as candidate relevant documents.

This report describes the logic and implementation of our solution, presents the first set of results we got from analysing articles from two online newspapers, and discusses the problems we faced and possible future research directions.   
\end{abstract}


\section{Introduction}
Publishing readers' comments on newspaper articles has been a long-standing tradition in good journalism since the early days of newspaper. Sometimes such  comments convey readers' feelings about the original article, sometimes they provide complementary information that help other readers to get a holistic understanding of the discussed topics, sometimes they expose errors, omissions, or subtle bias on the part of the reporter, and so on. Publication of readers' comments has, therefore, become an integral part of transparency in journalism.

During the era of printed newspapers, these comments used to appear as follow-up  discussions in days following the date of original article publishing. Because of space constraints, follow-up comments tend to be limited to only a few interesting news and furthermore limited in their numbers. The advent of electronic newspapers, however, has fundamentally altered that pre-existing culture. Currently, comments can be written on every published articles, by anyone interested,  and there is no limits in the number of comments either. Now it is quite common to see several hundreds of comments on popular news articles of papers like New York Times, BBC News, etc.

In regards to fact finding and ensuring transparency this is an encouraging trends. The sheer volume of such comments, however, introduces several important  challenges. On one hand, it can be overwhelming or completely unreasonable for a common reader to go through all the comments. On the other hand, for the editor it is difficult to highlight comments that may be most relevant to the  original article or purge comments that are simply vile. To the best of our  knowledge, so far little has been tried to aid both parties in this regard by automatic highlighting, filtering, and classification of readers' comments.

Although our broad objective for this research was to address all of the above, as part of the class project we focus on filtering relevant comments only. The central idea behind our solution is to view an article title as a query and the article itself as a feedback document for constructing the language model for the query intended by the title. Then to judge relevance of comments to the article or passages in the article, we automatically generate queries from the constructed model and rank comments against them.       

For our experiment we crawled about 1000 articles each having at least 20 comments from Alzajeera News and collected 780 more similarly comment-rich articles from Yahoo-News gathered by authors of \cite{Das:2014:GBC:2556195.2556231}. Language models for articles are uni-gram models that we generated using our own algorithm that relies on a notion of term significance which depends on both frequency and proximity of terms related to terms in the article title. Then a corpus based smoothing is applied to determine final term significance rankings. In short, a model in our case is not just a collection of terms; rather a probability estimate is associated with each term regarding its chance to be included in a query representing the information need served by the article or a part of that article. 

Ranking of comments against generated queries are done using Okapi BM25 \cite{Robertson96okapiat}. We varied different parameters of the ranking function as well as of our significance calculation to identify the best configuration. The ground truth for comments' relevance measurement is gathered through human evaluation of a small random sample of news from our crawled database.

The results are mixed. We identified several challenges regarding improving our solution further. Some problems were implementation specific issues that arose from simplifications we have to made to save time. Nevertheless, there are reasons to be optimistic and we hope to enhance our solution in the future.

The rest of the report is organized as follows. Section \ref{rw} discusses some related work; Section \ref{pf} discusses how we model our domain as an information retrieval problem; Section \ref{des} elaborates on aspects of our solution; Section \ref{si} briefs on the actual implementation; Section \ref{ev} presents the results of the experiments done on a small sample of articles; Section \ref{fw} ponders over possible routes for future work; finally, Section \ref{con} concludes the report.                

\section{Related Work}
\label{rw}
There have been a few works done on matching comments with News Article segments. Sil et. al. \cite{Sil:2011:SMC:2063576.2063906} used supervised and unsupervised techniques to create structural classifier to match comments with News article segments. This paper used explicit semantic analysis and co-reference features to represent the text in article and text and showed that accuracy of discriminative approaches depend largely on effective feature selection. They used prior data to detect article segment and comment match in articles of a related topic.

To detect article segment and comment matching we need to extract topics from a News article using topic modeling. Several works have been done in topic modeling. MG-LDA \cite{Blei:2003:MAD:860435.860460, Titov:2008:MOR:1367497.1367513} is used to model local topics in a text. However, In a news article every segment may not be related to a comment. Also in this model, local topics scatter across the corpus which is not often the case in news articles. Hence, MG-LDA is not appropriate for our system. Also, Corr-LDA \cite{Titov:2008:MOR:1367497.1367513} is a topic model to understand correspondence. Since, Corr-LDA work with single vector model, a specific comment on a small segment of an article can show small correlation with the article using this model. To overcome this limitation Das et. al. \cite{Das:2014:GBC:2556195.2556231} developed a correspondence topic model (SCTM) that uses multiple topic vectors. Hence, for each comment-article segment pair there would be number of topic vectors, which would enable comments to match with more correspondence segments of article. In \cite{Ma:2012:TRC:2396761.2396798} the authors focused on comments summarization. This paper selects few most representative comments from comment cluster for an article. They used Master-Slave Topic model (MSTM) and Extended Master-Slave Topic model (EXTM) where News articles is treated as master and comment text as slave. Using this topic models they cluster the comments based on their topics and rank the most representative comments from each comment clusters. 

\section{Problem Formulation}
\label{pf}
This section describes how we model the problem of finding relevant readers' comments as an information retrieval problem.

We view components of a news comprising the article and associated comments serve to satisfy a single information need that can be thought of as represented by a query that is similar to the title of the article. In our formulation, the comments are of interest only because the original article is by itself insufficient in conveying all information necessary to fully satisfy a reader's information need. 

Given this formation, the comments of interest are those comments that address topics covered in the article but augment article's author arguments. These comments broadly fall into to categories.  

\begin{enumerate}
\item Comments that support article author's arguments with additional information and insights, and
\item Comments that contradict the article with contrasting evidences and insights
\setcounter{tmpc}{\theenumi}
\end{enumerate}

Given the unregulated nature of comments publishing, the comments section is cluttered with several other kinds of comments such as (as we observed)

\begin{enumerate}
\setcounter{enumi}{\thetmpc}
\item Comments that express nothing but the sentiment of a particular reader about discussed topics
\item Arguments exchanging among readers about each other's comments
\item Elaborate discussion on some offshoot topic that has minute relevance to the article itself
\end{enumerate} 

An abundance of comments of latter categories makes it difficult to for an unbiased reader to identify and consequently satisfy his partially fulfilled information need. Although distinguishing between the former and latter kinds of comments is often a subjective decision, henceforth difficult, we hold that comments of former categories are more likely to have the following characteristic that will distinguish them from the latter.
\bigskip

\textit{There is significantly larger overlapping between principal issues discussed in an article and in a relevant comment than between the article and a non-relevant comment.}    
\bigskip

Given this assumption, the problem of filtering relevant comments can be designed as a standard information retrieval problem that has two parts.

\begin{enumerate}
\item Construct query language models from passages of an article 
\item Generate queries from these models and rank comments against individual queries
\end{enumerate}

Here we consider passage specific models as for articles having many passages, there may be jump in the topics been discussed. Consequently, a comment may also have passage specific complementary discussions. In the general case, the article presents a composite language model that is a mixture of the models of individual passages; and in the degenerative base case, it represents a single passage itself.  

Before we dive into a discussion about our implementation, we should emphasize that our language model should not be equate to a topic model similar to some related work discussed previously. Rather, it is intended to generate queries. To clarify the difference with an example if ``Obama decided to go alone in the Immigration reform issue" is a title of a news article then from the topic modelling perspective ``Immigration Reform'' is the primary -- and arguably the sole -- topic but from the query generation perspective ``Obama's Decision'' alongside ``Immigration Reform'' should construe the crux of the discussion.    

\section{Design}
\label{des}
The center piece of our solution is the algorithm for developing query language models for individual passages of a newspaper article. Next we discuss the algorithm and. Afterwords we discuss how these models can be used to generate queries that would be used for ranking evaluation.   

\subsection{Query Model Generation}
Given that we treat an article title to be equivalent to a query representing readers' information needs, we have a mechanism to interpret the significance of the content of the article. We believe a reasonable interpretation for any article is that each passage provides a partial information about the issue underlying the title and each sentence within a passage works on building up the arguments of its passage. To summarize, their is no term in the article that is not relevant to readers' information need.     

The relevance of these terms are, however, different and based on several parameters given below. 

\begin{enumerate}
\item Term Frequency
\item Significance Level: minimum distance of a term from terms occurring in the title
\item Relevance Weight: the number of co-occurrences of a term with other significant terms in the article 
\item Smoothing Factor: general likelihood of the term within the corpus      
\end{enumerate}

Among these the first and the last are self explanatory. So we focus on explaining the remaining two.

To calculate both significance level and relevance weight of terms we construct a graph ${G = (V, E)}$ for each passage $P$ where the vertex set $V = \{w_i | w_i \in P\}$ and the edge set $E = \{(w_i, w_j) | w_i, w_j \in s_k, s_k \in P\}$. That is there is an edge between two words if they occur in the same sentence in anywhere in the passage.  

Then the formula for the significance level of a word $w_i$ is as follows, 
\begin{equation}
\begin{array}{lr}
s_i = 1 : w_i \in title \\
s_i = 1 + distance_{min}(w_i, w_j) | w_j \in title) : w_i \notin title
\end{array}
\end{equation}
Here distance is measured as the number of edges between the concerned words pair.  

The formula for the relevance weight of a word $w_i$ is as follows, 
\begin{equation}
\label{rwEq}
rWeight_i = \sum_{\substack{(w_i, w_j) \in E\\s_j < s_i}} max\big(0, 1 - \log(s_j)\big)
\end{equation}
 

If we reason about aforementioned formulas, we see that terms that occurred at close proximity of any terms in the title have more significance than terms that appeared further. Then terms that occurred more often and with more significant terms have more relevance weight than terms that occurred less often.  

The rationale for this treatment lies in the assumption about the logical flow of arguments that we expect to present in a newspaper article. Sentences making statements about elements of the title should compose the main agenda. Then there will be sentences providing supporting evidences and so on. Consequently, terms occurring in different kind of sentences should have varying degrees of importance in the constructed model.   

Note that in Equation \ref{rwEq} only terms with lesser significance, $w_i$, get the benefit of co-occurrence -- not the term with greater significance, $w_j$. This apparent lacking is balanced by the frequency weight of individual term which has the following formula.

\begin{equation}
fWeight_i = tf \times {\frac{s_i + k}{s_i (k + 1)}}
\end{equation}   
Here $k$ is a scaling parameter used to exponentially decrease the importance of a term's occurrences with increase of its significance level.
The likelihood of a term to contribute to a query representing the information need satisfied by the concerned passage of the article is calculated as a mixture of normalized relevance and frequency weight as captured by the following equation.

\begin{equation}
\label{docW}
weight_i = \alpha \times rWeight_i + (1 - \alpha) \times fWeight_i
\end{equation}

Equation \ref{docW} is combined with the collection probability of a terms $p(w_i, C)$ using the Jelinek-Mercer interpolated smoothing method \cite{1371580} to give the final smoothed probability as follows.   

\begin{equation}
\label{wProb}
p_q(w_i) = (1 - \lambda) \times weight_i + \lambda \times p(w_i, C) 
\end{equation}

\subsection{Ranking Evaluation}
If we remember the discussion of Section \ref{pf}, we see the criteria for judging relevance of a comment to the discussion presented in the article or a passage of the article is the degree of overlapping of comment's language model with that of the former. Here \textit{overlapping} deserves careful attention.

Note that we expect from a relevant comment to complement the discussion of of the article with supporting or contradictory arguments. We do not expect the comment to say the same thing the article does. So there should be significant overlapping in principal terms of the comment and the article, but there should be also considerable distinction in their auxiliary terms. Therefore, ideally the goal of comments ranking should not be to maximize overlapping. Rather it should be to balance similarity in principal terms with dissimilarity in auxiliary terms.

If we define a cut-off threshold for partitioning the query language model of a passage $P$ into a principal $\theta_{pri}$ and an auxiliary part $\theta_{aux}$ then the relevance judgement for a comment $D$ with respect to the passage can be expressed as a harmonic mean as follows.

\begin{equation}
\label{relMes}
relevance(P,D) = \frac{1}{\frac{\beta}{sim(\theta_{pri}, \theta_d)} - \frac{1 - \beta}{sim(\theta_{aux}, \theta_d)}}
\end{equation}           

This is a generic equation and any standard metric -- for example KL-divergence -- can be substituted as the similarity measure to calculate the relevance in the above.

A reasonable simplification to Equation \ref{relMes} is to consider only the similarity between the comment model and passage model in the principal terms. This is practical as it is highly unlikely that a comment will only reiterate the argument presented in an article. Hence we can ignore the dissimilarity term and rank comments based on the following formula.

\begin{equation}
rank(P,D) \sim sim(\theta_{pri}, \theta_d)
\end{equation} 

\section{Implementation}
\label{si}

\section{Evaluation}
\label{ev}

\section{Future Work}
\label{fw}
 
\section{Conclusion}
\label{con}

\nocite{titov2008modeling, ma2012topic, das2014going, blei2003modeling}
\bibliographystyle{abbrv}
\bibliography{bibliography} 

\end{document}
